{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import densenets.dense_correspondence_manipulation.utils.utils as utils\n",
    "import torch\n",
    "from densenets.dataset.spartan_dataset_masked import SpartanDataset\n",
    "from densenets.evaluation.evaluation import DenseCorrespondenceEvaluation\n",
    "from densenets.training.training import DenseCorrespondenceTraining\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the configuration for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SpartanDataset:\n",
      "   - intrainmode\n",
      "   - number of scenes11\n",
      "   - total images:    2851\n"
     ]
    }
   ],
   "source": [
    "config_filename = os.path.join(\n",
    "    utils.getDenseCorrespondenceSourceDir(),\n",
    "    'config',\n",
    "    'dense_correspondence',\n",
    "    'dataset',\n",
    "    'composite',\n",
    "    'caterpillar_upright.yaml',\n",
    ")\n",
    "config = utils.getDictFromYamlFilename(config_filename)\n",
    "\n",
    "train_config_file = os.path.join(\n",
    "    utils.getDenseCorrespondenceSourceDir(),\n",
    "    'config',\n",
    "    'dense_correspondence',\n",
    "    'training',\n",
    "    'training.yaml',\n",
    ")\n",
    "\n",
    "train_config = utils.getDictFromYamlFilename(train_config_file)\n",
    "dataset = SpartanDataset(config=config)\n",
    "\n",
    "logging_dir = \"trained_models/tutorials\"\n",
    "num_iterations = 3500\n",
    "d = 3  # the descriptor dimension\n",
    "name = \"caterpillar_%d\" % (d)\n",
    "train_config[\"training\"][\"logging_dir_name\"] = name\n",
    "train_config[\"training\"][\"logging_dir\"] = logging_dir\n",
    "train_config[\"dense_correspondence_network\"][\"descriptor_dimension\"] = d\n",
    "train_config[\"training\"][\"num_iterations\"] = num_iterations\n",
    "\n",
    "TRAIN = True\n",
    "EVALUATE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Train the network\n",
    "\n",
    "This should take about ~12-15 minutes with a GTX 1080 Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__()\n",
    "        self.layer = torch.nn.Linear(1, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test(\n",
       "  (layer): Linear(in_features=1, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9260, -0.3454,  0.0805, -0.6740, -0.1332,  0.4803, -0.4895,  0.7961,\n",
       "         -0.6849,  0.6011],\n",
       "        [-0.9161, -0.3965, -0.1115, -0.5004, -0.0239,  0.2983, -0.6255,  0.8813,\n",
       "         -0.4915,  0.6820],\n",
       "        [-0.9045, -0.4570, -0.3385, -0.2952,  0.1053,  0.0831, -0.7863,  0.9822,\n",
       "         -0.2628,  0.7777],\n",
       "        [-0.8988, -0.4863, -0.4484, -0.1958,  0.1678, -0.0210, -0.8641,  1.0310,\n",
       "         -0.1521,  0.8240],\n",
       "        [-0.9282, -0.3336,  0.1247, -0.7140, -0.1583,  0.5222, -0.4582,  0.7764,\n",
       "         -0.7294,  0.5825],\n",
       "        [-0.9338, -0.3048,  0.2328, -0.8117, -0.2198,  0.6246, -0.3816,  0.7284,\n",
       "         -0.8383,  0.5370],\n",
       "        [-0.9266, -0.3420,  0.0932, -0.6855, -0.1404,  0.4923, -0.4805,  0.7904,\n",
       "         -0.6977,  0.5958],\n",
       "        [-0.9105, -0.4256, -0.2204, -0.4019,  0.0381,  0.1950, -0.7026,  0.9297,\n",
       "         -0.3817,  0.7279],\n",
       "        [-0.9362, -0.2924,  0.2793, -0.8537, -0.2463,  0.6687, -0.3487,  0.7078,\n",
       "         -0.8852,  0.5174],\n",
       "        [-0.9063, -0.4474, -0.3024, -0.3278,  0.0847,  0.1173, -0.7607,  0.9661,\n",
       "         -0.2991,  0.7625],\n",
       "        [-0.9186, -0.3836, -0.0631, -0.5442, -0.0514,  0.3441, -0.5912,  0.8599,\n",
       "         -0.5402,  0.6617],\n",
       "        [-0.9307, -0.3209,  0.1723, -0.7570, -0.1854,  0.5673, -0.4245,  0.7553,\n",
       "         -0.7774,  0.5625],\n",
       "        [-0.9056, -0.4510, -0.3158, -0.3157,  0.0923,  0.1047, -0.7702,  0.9721,\n",
       "         -0.2857,  0.7681],\n",
       "        [-0.9379, -0.2836,  0.3124, -0.8837, -0.2652,  0.7001, -0.3252,  0.6931,\n",
       "         -0.9186,  0.5034],\n",
       "        [-0.9178, -0.3877, -0.0783, -0.5305, -0.0428,  0.3298, -0.6019,  0.8666,\n",
       "         -0.5249,  0.6680],\n",
       "        [-0.9365, -0.2907,  0.2857, -0.8596, -0.2500,  0.6748, -0.3441,  0.7049,\n",
       "         -0.8917,  0.5147],\n",
       "        [-0.8987, -0.4868, -0.4503, -0.1941,  0.1689, -0.0229, -0.8655,  1.0319,\n",
       "         -0.1501,  0.8248],\n",
       "        [-0.9128, -0.4140, -0.1770, -0.4412,  0.0134,  0.2362, -0.6719,  0.9104,\n",
       "         -0.4255,  0.7096],\n",
       "        [-0.8986, -0.4877, -0.4537, -0.1910,  0.1708, -0.0261, -0.8679,  1.0333,\n",
       "         -0.1467,  0.8262],\n",
       "        [-0.9120, -0.4179, -0.1917, -0.4279,  0.0218,  0.2222, -0.6823,  0.9170,\n",
       "         -0.4106,  0.7158]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(torch.rand([20, 1]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading pose data for scene 2018-04-10-16-02-59\n",
      "INFO:root:Loading pose data for scene 2018-04-10-16-04-10\n",
      "INFO:root:Loading pose data for scene 2018-04-10-16-06-26\n",
      "INFO:root:Loading pose data for scene 2018-04-10-16-12-21\n",
      "INFO:root:Loading pose data for scene 2018-04-10-16-13-37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training descriptor of dimension 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading pose data for scene 2018-04-10-16-16-15\n",
      "INFO:root:Loading pose data for scene 2018-04-16-14-25-19\n",
      "INFO:root:Loading pose data for scene 2018-04-16-14-40-25\n",
      "INFO:root:Loading pose data for scene 2018-04-10-16-08-46\n",
      "INFO:root:Loading pose data for scene 2018-04-16-14-44-53\n",
      "INFO:root:Loading pose data for scene 2018-04-16-15-23-41\n",
      "INFO:root:enabling domain randomization\n",
      "INFO:root:setting up tensorboard_logger\n",
      "INFO:root:tensorboard logger started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using SINGLE_OBJECT_WITHIN_SCENE\n",
      "logging_dir:/home/stannis/Downloads/dense_correspondence_data/pdc/trained_models/tutorials/caterpillar_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stannis/ltr/sandbox/stannis/densenets/densenets/dense_correspondence_manipulation/utils/utils.py:345: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (flat_pixel_locations % image_width, flat_pixel_locations // image_width)\n",
      "/home/stannis/ltr/sandbox/stannis/densenets/densenets/dense_correspondence_manipulation/utils/utils.py:345: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (flat_pixel_locations % image_width, flat_pixel_locations // image_width)\n",
      "/home/stannis/ltr/sandbox/stannis/densenets/densenets/dense_correspondence_manipulation/utils/utils.py:345: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (flat_pixel_locations % image_width, flat_pixel_locations // image_width)\n",
      "/home/stannis/ltr/sandbox/stannis/densenets/densenets/dense_correspondence_manipulation/utils/utils.py:345: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (flat_pixel_locations % image_width, flat_pixel_locations // image_width)\n",
      "/home/stannis/ltr/sandbox/stannis/densenets/densenets/dense_correspondence_manipulation/utils/utils.py:345: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (flat_pixel_locations % image_width, flat_pixel_locations // image_width)\n",
      "/home/stannis/miniconda3/envs/ndf_robot/lib/python3.7/site-packages/torch/nn/functional.py:3847: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 1.1997e+00\n",
      "LOSS: 6.6009e-01\n",
      "LOSS: 6.2890e-01\n",
      "LOSS: 6.1507e-01\n",
      "LOSS: 3.4687e-01\n",
      "LOSS: 2.8737e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6917/2315121402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"training descriptor of dimension {d}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseCorrespondenceTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"finished training descriptor of dimension {d}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ltr/sandbox/stannis/densenets/densenets/training/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, loss_current_iteration, use_pretrained)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LOSS: {loss.item():.4e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ndf_robot/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ndf_robot/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ndf_robot/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ndf_robot/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# All of the saved data for this network will be located in the\n",
    "# code/data/pdc/trained_models/tutorials/caterpillar_3 folder\n",
    "\n",
    "if TRAIN:\n",
    "    print(f\"training descriptor of dimension {d}\")\n",
    "    train = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\n",
    "    train.run()\n",
    "    print(f\"finished training descriptor of dimension {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network quantitatively\n",
    "\n",
    "This should take ~5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(logging_dir, name)\n",
    "model_folder = utils.convert_data_relative_path_to_absolute_path(model_folder)\n",
    "\n",
    "if EVALUATE:\n",
    "    DCE = DenseCorrespondenceEvaluation\n",
    "    num_image_pairs = 100\n",
    "    DCE.run_evaluation_on_network(model_folder, num_image_pairs=num_image_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `evaluation_quantitative_tutorial.ipynb` for a better place to display the plots."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
